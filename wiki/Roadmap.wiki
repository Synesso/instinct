#summary Instinct Roadmap

= Implementation Plan =

== Short List ==

  # Documentation
    # Finish introductory tutorial.
    # State-based expectation tutorial.
    # Document lifecycle - how to extend, how to get callbacks for special behaviour.
  # IntelliJ plugin
    # ~~Initial functionality~~
    # Hook into JUnit runner
    # Release on IDEA plugins site.
  # ~~Improvements to the verification API, possibly based on Hamcrest.~~
  # Complete of the auto test-double creation code.
  # Expectation API
    # ~~State-based expectations~~
    # Behaviour expectations
    # Boost's assertThrows, inc. @Exception annotation or @Specification(exception=RuntimeException.class)?
  # Implementation of additional markers - naming conventions & marker interfaces.
  # JUnit 3 integration
    # ~~Complete JUnit 3 integration (inc. refactoring classes & taking into main tree).~~
    # Fix concurrent modification error in JUnit 3 code.
    # JUnit 4 runner.
    # JUnit XML test output formatters, for integration of JUnit style XSL stylesheets.
  # Rationalisation
    # Remove need for @Context annotation (still allow it if needed), find specs, and just use classes.
    # Remove boost dependency
  # Build
    # Add simian
    # Self host all current tests
    # Test coverage to 100%

== Detailed ==

  # ~~Build infrastructure~~
    # ~~Code coverage~~
    # ~~Artifacts~~
    # Simian
  # ~~TeamCity integration~~
  # ~~Implement core annotations (context, specification)~~
  # ~~Implement suite aggregator~~
  # ~~Get example code working~~
  # ~~Figure out mocking implementation to use~~
  # Auto mocking (dummies, stubs & mocks)
    # Mock discovery
    # Dummy creation
    # Stub creation
    # Mock creation
  # Test coverage to 100%
  # ~~Retrofit Assert.assertX() in tests to use Hamcrest style assertions~~
  # Auto unique field creation (are these just stubs?)
  # Auto fixture/subject creation
  # Add naming conventions to all locators
  # ~~Come up with nomenclature (see terms.txt)~~
  # Expectation API
    # ~~State-based expectations~~
    # Boost's assertThrows
  # Auto discovery of naming conventions at runtime
  # ~~IntelliJ Plugin~~
  # ~~Ant runner~~
    # ~~Make test failures read more like rSpec, see: http://blogs.objectmentor.com/articles/2007/02/01/specs-vs-tests~~
  # ~~Revisit distribution mechanism, perhaps an all, src, what about dependencies?~~
  # ~~Add source to distribution Jar~~
  # ~~Produce Javadoc Jar as part of artifacts~~
  # Remove reliance on boost

= Markers =

How do you find things? Specifications, contexts, mocks, etc.

  * Naming conventions
  * Marker interfaces
  * ~~Annotations~~
  * Method signatures - "public void ...()" for classes in the test tree for example. May need classes to be marked as contexts.
  * Define naming conventions etc. in properties file (mappings between interface names and implementations for test subjects).
  * Remove duplicates found in locators, e.g. fields marked with annotation and naming convention.
  * Add @Provided or @Wired to have a field resolved using an IoC container.
  * @Fixtures have a lifecycle - init() & destroy() - maybe these are annotations & naming conventions also.
  * When finding fixtures find all fields which implement marker interfaces.
  * Ensure that methods that are marked as both Before and After fail (are there any other mutually exclusive methods?)

== Annotations ==

  * ~~Use @BehaviourContext annotation to denote context classes.~~
  * Include an @Fixture annotation to support auto creation of fixtures where able to.
    * If fixtures depend on mocks, use the mock types where possible
    * Perhaps have conditions places on the fixtures to denote how they should be created @Fixture(dependencies=AUTO_MOCK,AUTO_UNIQUE)
  * Do we even need behaviour context markers? Surely we can just find the classes containing specifications and execute them (with currently implemented checks)
  * Add a "description" attribute to the BehaviourContext annotation, to give a description to use when reporting (errors, reports, documentation, etc.)

== Naming Conventions ==

  * Unique fields (also want to be able to specify conditions, e.g. positive int, ranges, etc.)
  * Mocked fields (a mock that is verified)
  * Stub fields (just an instance)
  * Fixtures

= Integration =

  * IntelliJ IDEA plugin
  * Eclipse plugin?
  * Ant support
  * Maven plugin (or SureFire integration)
  * ~~Support for JUnit runners.~~

== IntelliJ IDEA Plugin ==

  * Add syntax highlighting of mocks, etc., remove built in warnings for fields that haven't been assigned a value.
  * Attempt to re-use JUnit plugin in IDE.
  * ~~Look at JBehave IDEA plugin.~~
  * Look at YourKit plugin for JUnit integration.
  * Intentions
    * x == y -> expect.that(x).sameInstance(y)
    * x.equals(y) -> expect.that(x).equals(y)
    * foo.bar() -> expect.that(foo).method("bar");
  * Create fields, when declaring a subject (see Rob's plugin). Fields marked as mocks, stubs, etc.

= Testing =

  * Allow/encourage more than one Context per class file
  * Provide UniqueTestUtil functionality, @Unique annotation & naming convention (are these just dummies?)
  * ~~Create a Verify class similar to Assert. This will allow static access to the checks (so that this just becomes a wrapper). Probably use [http://code.google.com/p/hamcrest/ Hamcrest] to do this.~~
  * ~~Context classes should not need to have a suffix "Context".~~
  * Provide closure style assertThrows checks.
  * Support grouping of contexts
    * Via annotation
    * Via marker interfaces, allow extension of base marker interfaces to define new groups
    * Add this into Ant support as option on aggregators (i.e. name of group to run)
    * Add this into IntelliJ plugin, perhaps as an option in run dialog
    * Does it make sense to add this to the Context annotation or specification annotation like TestNG?

= Mocking =

  * ~~Bundle a mocking library - probably JMock~~
  * Create automocker - dummies, stubs & mocks
    * Support automocking using an annotation (@Mock) and naming convention
    * Fields for automocking will require annotation or naming convention (& currently hold null value?)
  * Mock role names should be discovered by using the name of the field (in addition to standard JMock method).
  * ~~Possibly allow different mock types - nice, stubs, etc.~~
  * Should allow mocking to be done automatically or manually. Manual mocking should be explicitly allowed.
  * ~~Expose Mocker via static utility, should have the same hooks into the lifecycle (e.g. verification) as auto mocked fields.~~
  * Implementations ideas:
    * Use JMock style DSL. Wrap JMock classes thinly.
    * Perhaps allow plugable mocking implementations?
    * See [http://shareandenjoy.saff.net/2006/12/interface-imposterization.html this] for details on how jMock 2 does it.
  * Propagate mock verification errors on assertion failures (need to call Mocker.verify()).
  * Check mocking of toString() for classes that override Object.toString() (apparently a jMock 1.1 bug).
  * Collection all verification errors and print them as one, rather than bailing on first, report them as one.
  * On expectation failre, dump out values of dummy variables/fields (the context).
  * On expectation failure, dump out context of expected and actual.
  * Replace EasyMock class extensions with Objenisis.

= Aggregation =

  * ~~Hook into junit style runners for IDE integration~~
  * IntelliJ plugin to run tests individually (look at code from TestNG plugin - http://code.google.com/p/testng-idea/).
  * Add ordering to mock API.

= Functionality =

  * ~~Allow "naming conventions" to be added~~ (discover these at runtime to extend functionality).
  * Need some way to discover what methods to run for non-annotated classes. Provide a naming convention to do this.
  * ~~For classes with annotations, may not want to also add in specification methods discovered by naming conventions.~~

= Test Strategy =

  * Determine how to automatically test core code against example code (maybe seperate classloader). This could form a  "compatibility" suite.

= Implementation Details =

  * ~~Do we need to create a new instance of the context class for each specification method?~~
  * Remove the need for marking classes with a context. How do we then handle contexts that have inherited spec methods? If we find classes to run based on spec methods we shouldn't run parent classes also. Perhaps we don't run abstract classes? Keep a list? Does this apply only to aggregators? Does it matter?
  * Create a way to dynamically compose Expect implementations, and how they releate to checkers. So that callers can create expectation instances in code (see David Saff's comment in mailing list).

= Lifecycle =

  * Expose lifecycle (in specification runner) as an annotation on the context (like JUnit's RunWith). Use the default lifecycle unless overridden
  * Provide a callback (listener/handler) interface so that a class can register to be notified of lifecycle events. This would allow extension to enable features like those used in EasyDoc test code.

= Documentation =

  * Describe (using an Ant build) how to use the Ant runner with system properties to only run a build on a CI server (for example), or some other conditions
  * Write example using: [http://www.daveastels.com/articles/2006/08/26/one-expectation-per-example-a-remake-of-one-assertion-per-test One Expectation per Example: A Remake of "One Assertion Per Test"]
  * Document automocking capabilities:
    * Declaring an array or collection will fill that collection with mocks.

= Ideas =

  * JDave
  * Jmockit
  * VirtualMock
  * http://code.google.com/p/objenesis/