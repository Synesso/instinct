#summary Instinct Roadmap

= Implementation Plan =

== Short List ==

  # Finish introductory tutorial.
  # An IntelliJ plugin.
  # Improvements to the verification API, possibly based on Hamcrest.
  # Completion of the auto test-double creation code.
  # Implementation of additional markers - naming conventions & marker interfaces.
  # JUnit XML test output formatters, for integration of JUnit style XSL stylesheets.

== Detailed ==

  # ~~Build infrastructure~~
    # ~~Code coverage~~
    # Artifacts
  # ~~TeamCity integration~~
  # ~~Implement core annotations (context, specification)~~
  # ~~Implement suite aggregator~~
  # ~~Get example code working~~
  # ~~Figure out mocking implementation to use~~
  # Auto mocking (dummies, stubs & mocks)
    # Mock discovery
    # Dummy creation
    # Stub creation
    # Mock creation
  # Test coverage to 100%
  # Retrofit Assert.assertX() in tests to use Hamcrest style assertions
  # Auto unique field creation (are these just dummies?)
  # Auto fixture/subject creation
  # Add naming conventions to all locators
  # ~~Come up with nomenclature (see terms.txt)~~
  # Verification classes (Hamcrest?)
    # JUnit Assert feature parity
    # Boost's assertThrows
  # Auto discovery of naming conventions at runtime
  # IntelliJ Plugin
  # ~~Ant runner~~
    # Make test failures read more like rSpec, see: http://blogs.objectmentor.com/articles/2007/02/01/specs-vs-tests
  # Revisit distribution mechanism, perhaps an all, src, what about dependencies?
  # ~~Add source to distribution Jar~~
  # Produce Javadoc Jar as part of artifacts
  # Remove reliance on boost(?)

= Markers =

How do you find things? Specifications, contexts, mocks, etc.

  * Naming conventions
  * Marker interfaces
  * ~~Annotations~~
  * Method signatures - "public void ...()" for classes in the test tree for example. May need classes to be marked as contexts.
  * Define naming conventions etc. in properties file (mappings between interface names and implementations for test subjects).
  * Remove duplicates found in locators, e.g. fields marked with annotation and naming convention.
  * Add @Provided or @Wired to have a field resolved using an IoC container.
  * @Fixtures have a lifecycle - init() & destroy() - maybe these are annotations & naming conventions also.
  * When finding fixtures find all fields which implement marker interfaces.
  * Ensure that methods that are marked as both Before and After fail (are there any other mutually exclusive methods?)

== Annotations ==

  * ~~Use @BehaviourContext annotation to denote context classes.~~
  * Include an @Fixture annotation to support auto creation of fixtures where able to.
    * If fixtures depend on mocks, use the mock types where possible
    * Perhaps have conditions places on the fixtures to denote how they should be created @Fixture(dependencies=AUTO_MOCK,AUTO_UNIQUE)
  * Do we even need behaviour context markers? Surely we can just find the classes containing specifications and execute them (with currently implemented checks)
  * Add a "description" attribute to the BehaviourContext annotation, to give a description to use when reporting (errors, reports, documentation, etc.)

== Naming Conventions ==

  * Unique fields (also want to be able to specify conditions, e.g. positive int, ranges, etc.)
  * Mocked fields (a mock that is verified)
  * Stub fields (just an instance)
  * Fixtures

= Integration =

  * IntelliJ IDEA plugin
  * Eclipse plugin?
  * Ant support
  * Maven plugin (or SureFire integration)
  * ~~Support for JUnit runners.~~

== IntelliJ IDEA Plugin ==

  * Add syntax highlighting of mocks, etc., remove built in warnings for fields that haven't been assigned a value.
  * Look to [http://code.google.com/p/agileplugins/ Rob's HotPlugin] for details on how to add runners.
  * Attempt to re-use JUnit plugin in IDE.

= Testing =

  * ~~Allow/encourage more than one Context per class file~~
  * Provide UniqueTestUtil functionality, @Unique annotation & naming convention (are these just dummies?)
  * Create a Verify class similar to Assert. This will allow static access to the checks (so that this just becomes a wrapper). Probably use [http://code.google.com/p/hamcrest/ Hamcrest] to do this.
  * ~~Context classes should not need to have a suffix "Context".~~
  * Provide closure style assertThrows checks.
  * Support grouping of contexts
    * Via annotation
    * Via marker interfaces, allow extension of base marker interfaces to define new groups
    * Add this into Ant support as option on aggregators (i.e. name of group to run)
    * Add this into IntelliJ plugin, perhaps as an option in run dialog
    * Does it make sense to add this to the BehaviourContext annotation or specification annotation like TestNG?

= Mocking =

  * Bundle a mocking library - probably JMock
  * Create automocker - dummies, stubs & mocks
    * Support automocking using an annotation (@Mock) and naming convention
    * Fields for automocking will require annotation or naming convention (& currently hold null value?)
  * Mock role names should be discovered by using the name of the field (in addition to standard JMock method).
  * ~~Possibly allow different mock types - nice, stubs, etc.~~
  * Should allow mocking to be done automatically or manually. Manual mocking should be explicitly allowed.
  * ~~Expose Mocker via static utility, should have the same hooks into the lifecycle (e.g. verification) as auto mocked fields.~~
  * Implementations ideas:
    * Use JMock style DSL. Wrap JMock classes thinly.
    * Perhaps allow plugable mocking implementations?
    * See [http://shareandenjoy.saff.net/2006/12/interface-imposterization.html this] for details on how jMock 2 does it.
  * Propagate mock verification errors on assertion failures (need to call Mocker.verify()).
  * Check mocking of toString() for classes that override Object.toString() (apparently a jMock 1.1 bug).

= Aggregation =

  * ~~Hook into junit style runners for IDE integration~~
  * IntelliJ plugin to run tests individually (look at code from TestNG plugin - http://code.google.com/p/testng-idea/).
  * Add ordering to mock API.

= Functionality =

  * ~~Allow "naming conventions" to be added~~ (discover these at runtime to extend functionality).
  * Need some way to discover what methods to run for non-annotated classes. Provide a naming convention to do this.
  * ~~For classes with annotations, may not want to also add in specification methods discovered by naming conventions.~~

= Test Strategy =

  * Determine how to automatically test core code against example code (maybe seperate classloader). This could form a  "compatibility" suite.

= Implementation Details =

  * Do we need to create a new instance of the context class for each specification method?

= Documentation =

  * Describe (using an Ant build) how to use the Ant runner with system properties to only run a build on a CI server (for example), or some other conditions

= Ideas =

  * JDave
  * Jmockit
  * VirtualMock
  * http://code.google.com/p/objenesis/